{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PCA Demo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example uses the [_Iris_](http://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html) dataset.\n",
    "This dataset consists of the petal and sepal lengths of three species of _Iris_ (_I. setosa_, _I. versicolor_, _I. virginica_).\n",
    "\n",
    "<img src=\"images/Iris_germanica.jpg\" width=\"200\" />\n",
    "\n",
    "> Petals are modified leaves that surround the reproductive parts of flowers.\n",
    "> Petals are usually accompanied by another set of modified leaves called sepals.\n",
    "\n",
    "<img src=\"images/petal_sepal.jpg\" width=\"200\" />\n",
    "\n",
    "The Iris dataset contains measurements for 150 iris flowers from these three different species (50 measurements per species).\n",
    "\n",
    "The four features in the Iris dataset are:\n",
    "\n",
    "1. sepal length in cm\n",
    "2. sepal width in cm\n",
    "3. petal length in cm\n",
    "4. petal width in cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be loading the dataset directly from the [UCI machine learning repository](https://archive.ics.uci.edu/dataset/53/iris):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget --no-check-certificate https://archive.ics.uci.edu/static/public/53/iris.zip\n",
    "! mkdir -pv data/iris/\n",
    "! unzip iris.zip -d data/iris/\n",
    "! rm iris.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    filepath_or_buffer=\"data/iris/iris.data\",\n",
    "    header=None,\n",
    "    sep=\",\",\n",
    ")\n",
    "df.columns = [\"sepal_len\", \"sepal_wid\", \"petal_len\", \"petal_wid\", \"class\"]\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We organize the data table into the data matrix $\\mathbf{X}$ and class labels $\\mathbf{y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, 0:4].values\n",
    "y = df.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our iris dataset is now stored in form of a $150 \\times 4$ matrix $\\textbf{X}$ where the columns are the different features, and every row represents a separate flower sample.\n",
    "Each sample row $\\textbf{x}$ can be pictured as a 4-dimensional vector:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^T =\n",
    "\\begin{pmatrix}\n",
    "    x_1 \\\\\n",
    "    x_2 \\\\\n",
    "    x_3 \\\\\n",
    "    x_4\n",
    "\\end{pmatrix} =\n",
    "\\begin{pmatrix}\n",
    "    \\text{sepal length} \\\\\n",
    "    \\text{sepal width} \\\\\n",
    "    \\text{petal length} \\\\\n",
    "    \\text{petal width}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "\n",
    "features = {\n",
    "    0: \"sepal length [cm]\",\n",
    "    1: \"sepal width [cm]\",\n",
    "    2: \"petal length [cm]\",\n",
    "    3: \"petal width [cm]\",\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "for cnt in features.keys():\n",
    "    plt.subplot(2, 2, cnt + 1)\n",
    "    for label in labels:\n",
    "        plt.hist(\n",
    "            x=X[y == label, cnt],\n",
    "            label=label,\n",
    "            bins=10,\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    plt.xlabel(xlabel=features[cnt])\n",
    "    plt.legend(loc=\"upper right\", fancybox=True, fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Standardizing**: standardize the data (zero mean, unit variance)\n",
    "2. **Eigendecomposition**: obtain the eigenvectors and eigenvalues from the covariance matrix or perform singular value decomposition of the data matrix\n",
    "3. **Selection of principal components**: sort eigenvalues in descending order and choose the $k$ eigenvectors that correspond to the $k$ largest eigenvalues where $k$ is the number of dimensions of the new feature subspace.\n",
    "4. **Construction of the projection matrix**: construct the projection matrix from the selected $k$ eigenvectors.\n",
    "5. **Transformation into new feature subspace**: transform the original data using the projection matrix to a $k$-dimensional feature subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Standardizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether to standardize the data prior to a PCA on the covariance matrix depends on the measurement scales of the original features.\n",
    "Since PCA yields a feature subspace that maximizes the variance along the axes, it makes sense to standardize the data, especially, if it was measured on different scales.\n",
    "Although all features in the Iris dataset were measured in centimeters, let us continue with the transformation of the data by removing the mean and scaling to unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = StandardScaler().fit_transform(X=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors and eigenvalues of a covariance matrix represent the \"core\" of a PCA: the eigenvectors (principal components) determine the directions of the new feature subspace, and the eigenvalues determine their magnitude.\n",
    "In other words, the eigenvalues explain the variance of the data along the new feature axes.\n",
    "\n",
    "The classic approach to PCA is to perform the eigendecomposition on the covariance matrix $\\Sigma$, where each element represents the covariance between two features.\n",
    "The covariance between two features is calculated as follows.\n",
    "Here we assume that the data already has a mean of zero.\n",
    "\n",
    "$$\n",
    "\\text{cov}({x_1,x_2}) =\n",
    "\\mathbb{E}_{\\text{x}_1\\sim{}P_{\\text{x}_1},\\text{x}_2\\sim{}P_{\\text{x}_2}}\n",
    "[x_1\\cdot x_2]\n",
    "$$\n",
    "\n",
    "High absolute values of the covariance mean that the values change a lot and at the same time are far from their mean values.\n",
    "If the sign of the covariance is positive, both random variables tend to take on relatively high values simultaneously.\n",
    "If the sign of the covariance is negative, then one random variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa.\n",
    "\n",
    "If the random variable pair $(\\text{x}_1,\\text{x}_2)$ can take on the values $\\left( x^{(i)}_1,x^{(i)}_2 \\right)$ for $i=1,\\dots m$ with equal probabilities $p_i = \\frac{1}{m}$, then the covariance can be written as:\n",
    "\n",
    "$$\n",
    "\\text{cov}({x_1,x_2}) =\n",
    "\\frac{1}{m} \\sum_{i} x^{(i)}_1 x^{(i)}_2\n",
    "$$\n",
    "\n",
    "We can summarize the calculation of the covariance matrix $\\mathbf{\\Sigma}$ via the following matrix equation.\n",
    "Note that we apply Bessel's correction (the use of $m-1$ instead of $m$ in the formula), which corrects the bias in the estimation of the population variance using the sample variance.\n",
    "\n",
    "$$\n",
    "\\Sigma = \\frac{1}{m-1} \\mathbf{X}^T \\mathbf{X}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the covariance matrix by hand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = (X_std).T.dot((X_std)) / (X_std.shape[0] - 1)\n",
    "print(f\"Covariance matrix:{os.linesep}{S}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by using NumPy's [`cov()`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html) function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Covariance matrix (NumPy):{os.linesep}{np.cov(m=X_std.T)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we perform an eigendecomposition on the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues, eigenvectors = np.linalg.eig(a=S)\n",
    "\n",
    "print(f\"Eigenvectors:{os.linesep}{eigenvectors}\")\n",
    "print(f\"Eigenvalues:{os.linesep}{eigenvalues}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> While the eigendecomposition of the covariance matrix may be more intuitive, most PCA implementations perform a singular value decomposition (SVD) of the *data matrix* (see lecture slides for why this is equivalent) to improve the computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Selection of Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to decide which eigenvector(s) can dropped without losing too much information for the construction of lower-dimensional feature subspace, we need to inspect the corresponding eigenvalues.\n",
    "The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.\n",
    "In order to do so, the common approach is to rank the eigenvalues from highest to lowest in order choose the top $k$ eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of (eigenvalue, eigenvector) tuples.\n",
    "eigenpairs = [\n",
    "    (np.abs(eigenvalues[i]), eigenvectors[:, i]) for i in range(len(eigenvalues))\n",
    "]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low.\n",
    "eigenpairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues.\n",
    "print(\"Eigenvalues in descending order:\")\n",
    "for i in eigenpairs:\n",
    "    print(i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sorting the eigenpairs, the next question is: \"How many principal components are we going to choose for our new feature subspace?\"\n",
    "A useful measure is the so-called \"explained variance\", which can be calculated from the eigenvalues.\n",
    "The explained variance tells us how much \"information\" can be attributed to each of the principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_explained_variance = [\n",
    "    (i / sum(eigenvalues)) * 100 for i in sorted(eigenvalues, reverse=True)\n",
    "]\n",
    "cumulative_explained_variance = np.cumsum(a=individual_explained_variance)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.bar(\n",
    "    x=range(len(eigenvalues)),\n",
    "    height=individual_explained_variance,\n",
    "    alpha=0.5,\n",
    "    align=\"center\",\n",
    "    label=\"Individual explained variance\",\n",
    ")\n",
    "plt.step(\n",
    "    x=range(len(eigenvalues)),\n",
    "    y=cumulative_explained_variance,\n",
    "    where=\"mid\",\n",
    "    label=\"Cumulative explained variance\",\n",
    ")\n",
    "plt.xlabel(\"Principal components\")\n",
    "plt.ylabel(\"Explained variance\")\n",
    "plt.xticks(\n",
    "    ticks=range(len(eigenvalues)), labels=[str(i) for i in range(len(eigenvalues))]\n",
    ")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above clearly shows that most of the variance (72.77%) can be explained by the first principal component alone.\n",
    "The second principal component still bears some information (23.03%) while the third and fourth principal components can safely be dropped without losing to much information.\n",
    "Together, the first two principal components contain 95.8% of the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Construction of the Projection Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we construct the projection matrix that will be used to transform the data onto the new feature subspace.\n",
    "The projection matrix is basically just a matrix of our concatenated top $k$ eigenvectors.\n",
    "\n",
    "Here, we are reducing the 4-dimensional feature space to a 2-dimensional feature subspace, by choosing the \"top 2\" eigenvectors with the highest eigenvalues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.hstack((eigenpairs[0][1].reshape(4, 1), eigenpairs[1][1].reshape(4, 1)))\n",
    "print(f\"W:{os.linesep}{W}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Transformation Into New Feature Subspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last step we will use the projection matrix $\\mathbf{W}$ to transform our samples onto the new subspace via the equation $\\textbf{Z} = \\textbf{X}\\textbf{W}$, where $\\textbf{Z}$ is a $150 \\times 2$ matrix of our transformed samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = X_std.dot(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for label, color in zip(\n",
    "    (\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"), (\"blue\", \"red\", \"green\")\n",
    "):\n",
    "    plt.scatter(Z[y == label, 0], Z[y == label, 1], label=label, c=color)\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(loc=\"lower center\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
