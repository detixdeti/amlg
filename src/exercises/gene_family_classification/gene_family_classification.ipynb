{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Classification of Gene Sequences Into Their Gene Families**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's download the data for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Append the root of the Git repository to the path.\n",
    "git_root = os.popen(cmd=\"git rev-parse --show-toplevel\").read().strip()\n",
    "sys.path.append(git_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import download_file\n",
    "\n",
    "if not os.path.exists(path=\"data\"):\n",
    "    os.makedirs(name=\"data\")\n",
    "\n",
    "download_file(\n",
    "    # url=\"https://dataverse.harvard.edu/api/access/datafile/10493562\",\n",
    "    url=\"https://seafile.cloud.uni-hannover.de/d/5d6029c6eaaf410c8b01/files/?p=%2Fgene_family_classification%2Fchimpanzee.txt&dl=1\",\n",
    "    save_filename=\"data/chimpanzee.txt\",\n",
    ")\n",
    "download_file(\n",
    "    # url=\"https://dataverse.harvard.edu/api/access/datafile/10493561\",\n",
    "    url=\"https://seafile.cloud.uni-hannover.de/d/5d6029c6eaaf410c8b01/files/?p=%2Fgene_family_classification%2Fdog.txt&dl=1\",\n",
    "    save_filename=\"data/dog.txt\",\n",
    ")\n",
    "download_file(\n",
    "    # url=\"https://dataverse.harvard.edu/api/access/datafile/10493560\",\n",
    "    url=\"https://seafile.cloud.uni-hannover.de/d/5d6029c6eaaf410c8b01/files/?p=%2Fgene_family_classification%2Fhuman.txt&dl=1\",\n",
    "    save_filename=\"data/human.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's group some imports here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import Any, List\n",
    "\n",
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Sequence Representation for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning models generally require input to be numerical values organized in the form of feature matrices.\n",
    "Biological sequence data is however usually provided in a string-like format.\n",
    "\n",
    "There are three basic approaches to represent (or in other words, encode) biological sequence data:\n",
    "\n",
    "1. Ordinal encoding\n",
    "2. One-hot encoding\n",
    "3. Biological sequences as a \"language\" through _k_-mer counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, however, we need a function that harmonizes biological sequence strings and converts them into more convenient `numpy` arrays:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Converting biological sequence strings to `numpy` arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function to harmonize (lowercase to uppercase, use `ACGTN` only) the biological sequence strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_array(sequence: str) -> npt.NDArray[Any]:\n",
    "    \"\"\"Convert a DNA sequence to a NumPy array.\"\"\"\n",
    "    # Harmonize lowercase and uppercase characters.\n",
    "    sequence = sequence.upper()\n",
    "\n",
    "    # Replace all characters in sequence that are not 'A', 'C', 'G', or 'T' with 'N'.\n",
    "    sequence = re.sub(pattern=\"[^ACGT]\", repl=\"N\", string=sequence)\n",
    "\n",
    "    # Format our output array.\n",
    "    array = np.array(list(sequence))\n",
    "    array = np.reshape(array, newshape=(len(array), 1))\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sequence_to_array(sequence=\"GATTACA\").T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, we encode bases (i.e., categorical features) as ordinal integers, i.e., integers having a relative ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "ordinal_encoder = OrdinalEncoder()\n",
    "ordinal_encoder.fit(X=np.array([[\"A\"], [\"C\"], [\"G\"], [\"T\"], [\"N\"]]))\n",
    "\n",
    "array = sequence_to_array(sequence=\"GATTACAN\")\n",
    "encoded_sequence = ordinal_encoder.transform(X=array)\n",
    "\n",
    "with np.printoptions(\n",
    "    formatter={\"float_kind\": lambda x: f\"{x:3.0f}\", \"str_kind\": lambda x: f\"  {x}\"}\n",
    "):\n",
    "    print(f\"Sequence:         {array.T}\")\n",
    "    print(f\"Encoded sequence: {encoded_sequence.T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approach is to use one-hot encoding to represent the biological sequence.\n",
    "This is widely used in deep learning methods.\n",
    "Each symbol gets transformed into a one-hot encoding vector.\n",
    "A one-hot encoding vector is a vector in which all elements are zero except for one element which is set to one.\n",
    "The position of the one in the vector encodes the symbol.\n",
    "For further processing, these one-hot encoded vectors can for example be concatenated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ One-hot encoding for biological sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the ordinal encoding, implement and perform the one-hot encoding for the sequence `GATTACA`.\n",
    "Use the `OneHotEncoder` class from `sklearn.preprocessing`.\n",
    "When instantiating an encoder from the class, use `sparse_output=False`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "onehot_encoder.fit(X=np.array([[\"A\"], [\"C\"], [\"G\"], [\"T\"], [\"N\"]]))\n",
    "\n",
    "array = sequence_to_array(sequence=\"GATTACA\")\n",
    "encoded_sequence = onehot_encoder.transform(X=array)\n",
    "\n",
    "with np.printoptions(\n",
    "    formatter={\"float_kind\": lambda x: f\"{x:3.0f}\", \"str_kind\": lambda x: f\"  {x}\"}\n",
    "):\n",
    "    print(f\"{array.T}\")\n",
    "    print(f\"{encoded_sequence.T}\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biological Sequences as a \"Language\" Through K-Mer Counting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hurdle that still remains is that none of these above methods results in arrays of uniform length, which is often a necessity.\n",
    "So with the above methods, you have to resort to things like truncating sequences or padding with \"N\" to get arrays of uniform length.\n",
    "\n",
    "Luckily there is the concept of _k_-mers.\n",
    "\n",
    "_k_-mers are substrings of length $k$ contained within a biological sequence.\n",
    "They are a fairly simple concept that turns out to be tremendously powerful.\n",
    "\n",
    "Usually, the term _k_-mer refers to all of a sequence's subsequences of length $k$.\n",
    "Using this definition, the sequence `ACGT` would have four monomers (`A`, `C`, `G`, `T`), three 2-mers (`AC`, `CG`, `GT`), two 3-mers (`ACG`, `CGT`), and one 4-mer (`ACGT`).\n",
    "More generally, a sequence of length $L$ has $L - k + 1$ _k_-mers and $n^k$ total possible _k_-mers, where $n$ is the number of possible monomers.\n",
    "\n",
    "In some sense, by using _k_-mers, we can also incorporate the concepts of \"words\" (_k_-mers) and \"sentences\" (sequences of _k_-mers) from natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function `build_kmers()` returns a list of _k_-mer \"words\" for a given sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kmers(sequence: str, k: int) -> List[str]:\n",
    "    \"\"\"Build k-mers from a sequence.\"\"\"\n",
    "    return [sequence[i : i + k] for i in range(len(sequence) - k + 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this function to generate a language-like sentence from a biological sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"GTGCCCAGGTTCAGTGA\"\n",
    "words = build_kmers(sequence=sequence, k=3)\n",
    "sentence = \" \".join(words)\n",
    "\n",
    "print(f\"Sequence : {sequence}\")\n",
    "print(f\"Words    : {words}\")\n",
    "print(f\"Sentence : {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we may use a **Bag of Words** (BoW) model to represent our \"sentence\".\n",
    "\n",
    "> A Bag of Words (BoW) model is a popular and simple way to represent text data in natural language processing (NLP) and information retrieval.\n",
    "> In this model, a document is represented as an unordered set of words, disregarding grammar and word order but keeping track of word frequency.\n",
    "> The name \"Bag of Words\" implies that the model is only concerned with the occurrence of words in a document, not with their order or structure.\n",
    "\n",
    "> Here's how the Bag of Words model typically works:\n",
    ">\n",
    "> 1. Tokenization: The first step is to break down a text into individual words or tokens. This process involves removing punctuation, converting all words to lowercase, and splitting the text into individual words.\n",
    "> 2. Vocabulary Building: Create a vocabulary of all unique words in the entire corpus (collection of documents). Each unique word in this vocabulary is assigned a unique index.\n",
    "> 3. Vectorization: Represent each document as a vector in the space of the vocabulary. The vector has the same length as the vocabulary, and each element of the vector corresponds to the frequency of a word from the vocabulary in the document.\n",
    "\n",
    "> Here's a simple example:\n",
    ">\n",
    "> Consider two documents:\n",
    ">\n",
    "> - Document 1: \"The cat in the hat.\"\n",
    "> - Document 2: \"The quick brown fox.\"\n",
    "> - The vocabulary is: {the, cat, in, hat, quick, brown, fox}.\n",
    ">\n",
    "> The Bag of Words representation for these documents would be:\n",
    ">\n",
    "> - Document 1: [2, 1, 1, 1, 0, 0, 0]\n",
    "> - Document 2: [1, 0, 1, 0, 1, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Employing the BoW model in the context of biological sequence data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BoW model can&nbsp;&mdash;&nbsp;analogous to its use in NLP&nbsp;&mdash;&nbsp;be employed for biological sequence data.\n",
    "\n",
    "1. Use the function `build_kmers()` to build a word corpus from the given sequence corpus. Use $k=3$.\n",
    "2. Use the `CountVectorizer` from `sklearn.feature_extraction.text` to fit the BoW model to the word corpus.\n",
    "3. Vectorize new \"documents\" using the trained BoW model.\n",
    "\n",
    "> Use the function `toarray()` to convert sparse matrix objects returned by `CountVectorizer`s `transform()` and `fit_transform()` functions into dense representation as a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_corpus = [\"GATTCCAG\", \"GATTACA\", \"NAAGT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the word corpus.\n",
    "\n",
    "word_corpus = [\n",
    "    build_kmers(sequence=sequence_corpus[i], k=3) for i in range(len(sequence_corpus))\n",
    "]\n",
    "word_corpus = [item for sublist in word_corpus for item in sublist]\n",
    "word_corpus = \" \".join(word_corpus)\n",
    "\n",
    "print(f\"Word corpus: {word_corpus}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the BoW model to the word corpus.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "word_vectorization = vectorizer.fit_transform(raw_documents=[word_corpus])\n",
    "\n",
    "print(f\"Word vectorization: {word_vectorization.toarray()}\")  # type: ignore\n",
    "print(f\"Feature names: {vectorizer.get_feature_names_out()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize new documents using the trained BoW model.\n",
    "\n",
    "print(vectorizer.transform([\"GAT ATT TTA TAC CAG\"]).toarray())  # type: ignore\n",
    "print(vectorizer.transform([\"GAT ATT ACA ACA ACA\"]).toarray())  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Real-World End-to-End Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this end-to-end exercise, the goal is to classify gene sequences into their gene families.\n",
    "\n",
    "We provide gene sequences from three species in the `data/` folder:\n",
    "\n",
    "- [Chimpanzee](https://en.wikipedia.org/wiki/Chimpanzee) (_Pan troglodytes_): `chimpanzee.txt`\n",
    "- [Dog](https://en.wikipedia.org/wiki/Dog) (_Canis familiaris_): `dog.txt`\n",
    "- [Human](https://en.wikipedia.org/wiki/Human) (_Homo sapiens_): `human.txt`\n",
    "\n",
    "Each file contains two tab-separated columns.\n",
    "The first column (column header: `sequence`) contains the relevant gene sequence.\n",
    "The second column (column header: `class`) contains the corresponding gene family, coded as integer:\n",
    "\n",
    "| Identifier | Gene family                         |\n",
    "|------------|-------------------------------------|\n",
    "|          0 | G protein-coupled receptors (GPCRs) |\n",
    "|          1 | Tyrosine kinase                     |\n",
    "|          2 | Tyrosine phosphatase                |\n",
    "|          3 | Synthetase                          |\n",
    "|          4 | Synthase                            |\n",
    "|          5 | Ion channel                         |\n",
    "|          6 | Transcription factor                |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Reading data in with `pandas`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pandas`' [`read_table()`](https://pandas.pydata.org/docs/reference/api/pandas.read_table.html) function to read in the three files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chimp_dna = pd.read_table(filepath_or_buffer=\"data/chimpanzee.txt\")\n",
    "dog_dna = pd.read_table(filepath_or_buffer=\"data/dog.txt\")\n",
    "human_dna = pd.read_table(filepath_or_buffer=\"data/human.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dna.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Building _k_-mer \"words\" for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences into short overlapping _k_-mer \"words\" of length $k=6$.\n",
    "For now, do this only for the human data.\n",
    "Add the _k_-mer words as additional column to the `human_dna` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_dna[\"words\"] = human_dna.apply(lambda x: build_kmers(x[\"sequence\"], k=6), axis=1)\n",
    "human_dna.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Building \"sentences\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the human data, convert the lists of _k_-mers into sentences of words (separated by one whitespace) that can be used to fit the BoW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_corpus = [\" \".join(words) for words in human_dna[\"words\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Fit a BoW model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `CountVectorizer` from `sklearn.feature_extraction.text` to fit a BoW model to the `human_corpus`.\n",
    "Also, vectorize the entire corpus.\n",
    "\n",
    "> Play around with the `ngram_range` argument of `CountVectorizer()` to introduce more context into the vectorizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "human_vectorizations = vectorizer.fit_transform(raw_documents=human_corpus)\n",
    "print(f\"Vectorizations shape: {human_vectorizations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Split the data into a training and a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the class labels from the `human_dna` dataframe.\n",
    "Then, use the function [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) from `sklearn.model_selection` to split the vectorizations _and_ the class labels into a training set (80%) and a test set (20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_human = human_dna[\"class\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    human_vectorizations, y_human, test_size=0.20, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ❓ Train a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a classifier of your choice to classify the data.\n",
    "\n",
    "> Try for example:\n",
    ">\n",
    "> - `sklearn.naive_bayes.MultinomialNB`\n",
    "> - `sklearn.tree.DecisionTreeClassifier`\n",
    "> - `sklearn.ensemble.RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X=X_train, y=y_train)\n",
    "y_pred = clf.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true=y_test, y_pred=y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(f\"Accuracy  : {accuracy_score(y_true=y_test, y_pred=y_pred):.4f}\")\n",
    "print(\n",
    "    f\"Precision : \"\n",
    "    f\"{precision_score(y_true=y_test, y_pred=y_pred, average='weighted'):.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Recall    : {recall_score(y_true=y_test, y_pred=y_pred, average='weighted'):.4f}\"\n",
    ")\n",
    "print(f\"F1 score  : {f1_score(y_true=y_test, y_pred=y_pred, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try to use different `max_depth`s for the `DecisionTreeClassifier`.\n",
    "- The classes are imbalanced. Try to upsample the minority classes using `resample()` from `sklearn.utils`. Also try to downsample the majority class(es).\n",
    "- It could be beneficial to tune some hyperparameters (e.g., `CountVectorizer`'s `ngram_range` parameter and the _k_-mer length)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
